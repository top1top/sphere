{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 4. Конструирование текстовых признаков из твитов пользователей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сбор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый этап - сбор твитов пользователей. Необходимо подключаться к Twitter API и запрашивать твиты по id пользователя. \n",
    "Подключение к API подробно описано в ДЗ 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "\n",
    "CONSUMER_KEY = \"JU3TRo8VDXB6521Y1z9dCkjtG\"\n",
    "CONSUMER_SECRET = \"Q9tFFbdEecuNXmKdovICMP0sZ5r4a6iglzzB8bywyIQtQXF7LK\"\n",
    "\n",
    "ACCESS_TOKEN_KEY = \"780472030546292736-B9OjRHVbOdPBomYYWpxa54vqHnFpvLZ\"\n",
    "ACCESS_TOKEN_SECRET = \"6VM3hyw2QM8iG1z9DY1uoPrcG76KkU5c9RaSKKT4v8me6\"\n",
    "\n",
    "api = twitter.Api(consumer_key=CONSUMER_KEY, \n",
    "                  consumer_secret=CONSUMER_SECRET, \n",
    "                  access_token_key=ACCESS_TOKEN_KEY, \n",
    "                  access_token_secret=ACCESS_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения твитов пользователя может быть использован метод GetUserTimeline из библиотеки python-twitter. Он позволяет получить не более 200 твитов пользователя. По каждому пользователю достаточно собрать 200 твитов.\n",
    "\n",
    "Метод имеет ограничение по количеству запросов в секунду. Для получения информации о промежутке времени, которое необходимо подождать для повторного обращения к API может быть использован метод `GetSleepTime`. Для получения информации об ограничениях запросов с помощью метода `GetUserTimeLine` необходимо вызывать `GetSleepTime` с параметром \"statuses/user_timeline\".\n",
    "\n",
    "Метод GetUserTimeline возвращает объекты типа Status. У этих объектов есть метод AsDict, который позволяет представить твит в виде словаря.\n",
    "\n",
    "Id пользователей необходимо считать из файла, как было сделано в ДЗ 1.\n",
    "\n",
    "Необходимо реализовать функцию `get_user_tweets(user_id)`. Входной параметр - id пользователя из файла. Возвращаемое значение - массив твитов пользователя, где каждый твит представлен в виде словаря. Предполагается, что информация о пользователе содержится в твитах, которые пользователь написал сам. Это означает, что можно попробовать отфильтровать ответы другим пользователям, ссылки и ретвиты, а так же картинки и видео, так как наша цель - найти текстовую информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_user_tweets(user_id):\n",
    "    reslist = []\n",
    "    try:\n",
    "        statuses = api.GetUserTimeline(user_id = user_id, count=200, trim_user=True, \n",
    "                                       exclude_replies=True, include_rts=False)\n",
    "        for status in statuses:\n",
    "            d = status.AsDict()\n",
    "            if not ('retweeted_status' in d) and (d['lang']=='en'):\n",
    "                res = {'lang': d['lang'], \n",
    "                 'favorited': d['favorite_count']>0 if 'favorite_count' in d else False, \n",
    "                 'truncated': d[\"truncated\"] if \"truncated\" in d else False, \n",
    "                 'text': d['text'], \n",
    "                 'created_at': d['created_at'], \n",
    "                 'retweeted': d['retweet_count']>0 if 'retweet_count' in d else False, \n",
    "                 'source': d['source'] if 'source' in d else None, \n",
    "                 'user': d['user'], \n",
    "                 'id': d['id']}\n",
    "                reslist.append(res)\n",
    "    except twitter.TwitterError as err:\n",
    "        if err.message==\"Rate limit exceeded\":\n",
    "            raise\n",
    "        if err.message!=\"Not authorized.\":\n",
    "            print \"\\nError: \",err\n",
    "        return reslist\n",
    "    return reslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбор текста твита"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка текста предполагает разбиение текста на отдельные элементы - параграфы, предложения, слова. Мы будем преобразовывать текст твита к словам. Для этого текст необходимо разбить на слова. Сделать это можно, например, с помощью регулярного выражения.\n",
    "\n",
    "Необходимо реализовать функцию, `get_words(text)`. Входной параметр - строка с текстом. Возвращаемое значение - массив строк (слов). Обратите внимание, что нужно учесть возможное наличие пунктуации и выделить по возможности только слова. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_words(text):\n",
    "    text = text.lower()\n",
    "    find = re.findall(r'[^\\\\=\\-?~$%&/\\[\\]+><{}_^(@)*#\\s!,.?\":;0-9]+', text)\n",
    "    return find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'are', 'different', 'words']\n"
     ]
    }
   ],
   "source": [
    "print get_words(\"Here are different words!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее полученные слова необходимо привести к нормальной форме. То есть привести их к форме единственного числа настоящего времени и пр. Сделать это можно с помощью библиотеки nltk. Информацию по загрузке, установке библиотеки и примерах использования можно найти на сайте http://www.nltk.org/\n",
    "\n",
    "Для загрузки всех необходимых словарей можно воспользоваться методом download из библиотеки nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Olenka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Olenka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "list_stopwords = stopwords.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для дальнейшей обработки слова должны быть приведены к нижнему регистру. \n",
    "\n",
    "Для приведения к нормальной форме можно использовать `WordNetLemmatizer` из библиотеки nltk. У этого класса есть метод `lemmatize`.\n",
    "\n",
    "Также необходимо убрать из текста так называемые стоп-слова. Это часто используемые слова, не несущие смысловой нагрузки для наших задач. Сделать это можно с помощью `stopwords` из nltk.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `get_tokens(words)`. Входной параметр - массив слов. Возвращаемое значение - массив токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tokens(words):\n",
    "    list_tokens = [wnl.lemmatize(word) for word in words if wnl.lemmatize(word) not in list_stopwords]\n",
    "    return list_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `get_tweet_tokens(tweet)`. Входной параметр - текст твита. Возвращаемое значение -- токены твита. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tweet_tokens(tweet):\n",
    "    return get_tokens(get_words(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `collect_users_tokens()`. Функция должна сконструировать матрицу признаков пользователей. В этой матрице строка - пользователь. Столбец - токен. На пересечении - сколько раз токен встречается у пользователя.\n",
    "Для построения матрицы можно использовать `DictVectorizer` из `sklearn.feature_extraction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_users_tokens(df_users):\n",
    "    \"\"\"returns users list and list of user dicts. Each dict contains frequence of user tokens\"\"\"\n",
    "    list_users = []\n",
    "    list_dict_tokens = []\n",
    "    # Noise found on tag-cloud\n",
    "    list_noise = ['http', 'don\\'t', 'i\\'m', 'it', 'it\\'s', 'amp']\n",
    "    \n",
    "    for user in df_users.uid:\n",
    "        user_tweets = get_user_tweets(user)\n",
    "        dict_tokens = {}\n",
    "        for tweet in user_tweets:\n",
    "            list_tokens = get_tweet_tokens(tweet[\"text\"])\n",
    "            for token in list_tokens:\n",
    "                if token in dict_tokens:\n",
    "                    dict_tokens[token] =  dict_tokens[token] + 1\n",
    "                else:\n",
    "                    if len(token) > 1 and token not in list_noise:\n",
    "                        dict_tokens[token] = 1\n",
    "        list_users.append(user)\n",
    "        list_dict_tokens.append(dict_tokens)\n",
    "        \n",
    "    return list_users, list_dict_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n",
      "\n",
      "Error:  [{u'message': u'Sorry, that page does not exist.', u'code': 34}]\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', BadStatusLine(\"''\",))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-a316c9005a62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mdf_users\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_users_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_users_ex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0musers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musers_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollect_users_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_users\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mvs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musers_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-89447828e245>\u001b[0m in \u001b[0;36mcollect_users_tokens\u001b[1;34m(df_users)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_users\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0muser_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_user_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mdict_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0muser_tweets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-f6231d9af4fc>\u001b[0m in \u001b[0;36mget_user_tweets\u001b[1;34m(user_id)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         statuses = api.GetUserTimeline(user_id = user_id, count=200, trim_user=True, \n\u001b[1;32m----> 5\u001b[1;33m                                        exclude_replies=True, include_rts=False)\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstatuses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAsDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Olenka\\Anaconda2\\lib\\site-packages\\twitter\\api.pyc\u001b[0m in \u001b[0;36mGetUserTimeline\u001b[1;34m(self, user_id, screen_name, since_id, max_id, count, include_rts, trim_user, exclude_replies)\u001b[0m\n\u001b[0;32m    713\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'exclude_replies'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 715\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_RequestUrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GET'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    716\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ParseAndCheckTwitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Olenka\\Anaconda2\\lib\\site-packages\\twitter\\api.pyc\u001b[0m in \u001b[0;36m_RequestUrl\u001b[1;34m(self, url, verb, data, json)\u001b[0m\n\u001b[0;32m   4770\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mverb\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'GET'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4771\u001b[0m             \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_BuildUrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4772\u001b[1;33m             \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__auth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4774\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Olenka\\Anaconda2\\lib\\site-packages\\requests\\api.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Olenka\\Anaconda2\\lib\\site-packages\\requests\\api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Olenka\\Anaconda2\\lib\\site-packages\\requests\\sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    473\u001b[0m         }\n\u001b[0;32m    474\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Olenka\\Anaconda2\\lib\\site-packages\\requests\\sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Olenka\\Anaconda2\\lib\\site-packages\\requests\\adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', BadStatusLine(\"''\",))"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "TRAINING_SET_URL = \"train.csv\"\n",
    "EXAMPLE_SET_URL = \"test.csv\"\n",
    "df_users_train = pd.read_csv(TRAINING_SET_URL, sep=\",\", header=0, names=[\"uid\", \"cls\"]\n",
    "df_users_ex = pd.read_csv(EXAMPLE_SET_URL, sep=\",\", header=0, names=[\"uid\"])\n",
    "df_users_ex['cls'] = None\n",
    "df_users = pd.concat([df_users_train, df_users_ex])\n",
    "\n",
    "users, users_tokens = collect_users_tokens(df_users)\n",
    "v = DictVectorizer()\n",
    "vs = v.fit_transform(users_tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Сохраним полученные данные в файл. Используется метод savez из numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savez(\"files/out_4.dat\", data=vs, users=users, users_tokens=users_tokens )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее для получения представления о полученной информацию о токенах предлагается отобразить ее в виде облака тэгов. [Подсказка](http://anokhin.github.io/img/tag_cloud.png). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def draw_tag_cloud(v, vs, users_tokens):\n",
    "    \"\"\"Draws tag cloud of found tokens\"\"\"\n",
    "    freqs = sum((Counter(dict(x)) for x in users_tokens), Counter()).items()\n",
    "    wc = WordCloud(max_words=1000, margin=10, background_color='white',\n",
    "        random_state=1).generate_from_frequencies(freqs)\n",
    "    default_colors = wc.to_array()\n",
    "    plt.figure()\n",
    "    plt.imshow(default_colors)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "draw_tag_cloud(v, vs, users_tokens)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
